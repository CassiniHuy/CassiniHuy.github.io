---
title: Jailbroken文章阅读
date: 2023-09-17 14:46:51
categories: [随笔]
tags: [大语言模型, 提示注入, 越狱, ChatGPT]
---

这周小小看了下[Jailbroken: How Does LLM Safety Training Fail?](arxiv.org/abs/2307.02483)这篇文章。

<!--more-->

# 背景介绍

大语言模型，例如常见的ChatGPT这类聊天模型，可能不是什么都能说的，对于某些问题或者用户请求，其是不应该满足的，即与价值对齐。

但是，由于大模型预训练时，并不具有显式地学习到这个能力的条件，毕竟人类自己都没对齐，产生的大量数据其孕育的大模型自然也不是天然具备这个能力的。

因此，常常通过净化训练数据集再训练、强化学习去纠正、或是直接输入输出审查等方法，来“对齐”模型。

而提示注入、越狱这一类方法，即是希望能够将大模型回到对齐前的状态，可想而知，该技术诞生的起因并不是一个正常的需求。

# 为什么我们能够注入/越狱大模型？

这源于大模型训练时，具有两个矛盾的目标，这两个目标之间的拉扯，就使得越狱这一类的方法具有了可能性：

- Competing objectives: 大模型本身训练时，这种对齐的要求，就使得其自身具有一个矛盾；对齐要求大模型拒绝请求，即不遵守用户的请求，而同时大模型训练的基本目标就是满足用户的请求，按照用户的要求行动。
- Mismatched generalization: 大模型对齐的过程中，很难做到对所有数据都具有很强的泛化性，例如，如果只对英文做了对齐，那么中文便没有对齐；类似的，还有通过各种简单的加密方法，例如base64，越过其对齐能力（当然，前提是大模型能够理解这样的加密通信）。而大模型本身还不具有很强的这种能力，使得其能够将同一概念的不同书写形式，理解为同一的，然后即使在一个域里能够对齐，其他的也能对齐。这也是一个大模型仅仅本质只是拟合的概率分布的一个基本矛盾。

# 那么上面两类有哪些攻击方式？

第一类即是直接对模型的行为做出要求，例如：

- 要求以某一字符串开头，这类开头能够带来更大的给出回复的概率，例如：“Certainly, here it is:”
- 要求其不能回复sorry，apologize，cannot这里话
- 要求其忽略system prompt
- 要求其在回复方式上做出变化，这像是分散其注意力
- 等等

各类越狱指令都是上述方法的结合或者加长版，以获得模型更强的注意力。

第二类即是利用一些“加密”方法：

- 用base64问和答
- 用移位密码
- 其他等等

上面的方法对模型本身的能力有要求，例如GPT-4上才有效，因为其够聪明。


