<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>nvidia-tensorflow 1.15.5的signal模块的一个内存泄漏bug | Cheng&#39;an Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="苟日新，日日新，又日新。">
<meta property="og:type" content="article">
<meta property="og:title" content="nvidia-tensorflow 1.15.5的signal模块的一个内存泄漏bug">
<meta property="og:url" content="http://weichengan.com/2023/01/09/suibi/nv_tf1155_bug/index.html">
<meta property="og:site_name" content="Cheng&#39;an Blog">
<meta property="og:description" content="苟日新，日日新，又日新。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-01-08T17:16:00.000Z">
<meta property="article:modified_time" content="2023-02-17T06:55:48.150Z">
<meta property="article:author" content="cassiniwei@outlook.com">
<meta property="article:tag" content="tensorflow 1.15">
<meta property="article:tag" content="nvidia">
<meta property="article:tag" content="内存泄露">
<meta property="article:tag" content="RTX 4090">
<meta name="twitter:card" content="summary">
  
  
    <link rel="icon" href="/images/favicon.ico">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  

  

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Cheng&#39;an Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" target="_blank" rel="noopener" href="https://space.bilibili.com/85520747">bilibili</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-github-link" class="nav-icon" href="https://github.com/CassiniHuy" title="Github" target="_blank"></a>
        
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://weichengan.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-suibi/nv_tf1155_bug" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    
<a href="/2023/01/09/suibi/nv_tf1155_bug/" class="article-date">
  <time class="dt-published" datetime="2023-01-08T17:16:00.000Z" itemprop="datePublished">2023-01-09</time>
</a>


    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      nvidia-tensorflow 1.15.5的signal模块的一个内存泄漏bug
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
		
		<div id="toc" class="toc-article">
			<h2 class="toc-title"><span>Contents</span></h2>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E7%9A%84%E5%8F%91%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text">问题的发现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%97%AE%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%91%A2%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">问问搜索引擎呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%AF%E6%98%BE%E5%8D%A1%E7%9A%84%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F"><span class="toc-number">3.</span> <span class="toc-text">是显卡的问题吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%AF%E9%95%9C%E5%83%8F%E7%9A%84%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F"><span class="toc-number">4.</span> <span class="toc-text">是镜像的问题吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%B0%E5%BA%95%E6%98%AF%E5%93%AA%E5%84%BF%E8%80%97%E6%97%B6%E5%9C%A8%E4%B8%80%E7%9B%B4%E5%A2%9E%E5%8A%A0%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">到底是哪儿耗时在一直增加？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%B0%E5%BA%95%E6%98%AF%E5%93%AA%E5%84%BF%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">到底是哪儿内存泄漏？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%88%E4%BA%8E%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%98%EF%BC%9A%E7%9C%9F%E7%9A%84%E6%98%AFnvidia-tensorflow%E7%9A%84bug%E5%90%97%EF%BC%9F%EF%BC%81"><span class="toc-number">7.</span> <span class="toc-text">终于发现问题：真的是nvidia-tensorflow的bug吗？！</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%B0%E5%BA%95%E6%80%8E%E4%B9%88%E5%8A%9E%E5%91%A2%EF%BC%9F"><span class="toc-number">8.</span> <span class="toc-text">到底怎么办呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">9.</span> <span class="toc-text">总结</span></a></li></ol>
		
		</div>
		
        <p>最近实验室新进了一台有RTX 4090的工作站，于是在这台工作站上跑起了以前tensorflow 1时代写的代码，用的是nvidia ngc提供的docker镜像运行。</p>
<p>但是，发现session执行时间一次比一次长，同时发现内存也在缓慢增长，reset_default_graph也不能阻止这个过程；并且这个问题只在新服务器上出现，旧的RTX 3090显卡的服务器并没有这个问题。</p>
<p>于是开始了三四天的排查。</p>
<p>提出issue在：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/tensorflow/issues/76">https://github.com/NVIDIA/tensorflow/issues/76</a></p>
<p>结论：tf.signal.rfft, tf.signal.dct等方法有bug，通过降低nvida-tensorflow的版本到1.15.4解决。</p>
<p>下面是排查思路和过程。</p>
<span id="more"></span>
<h2 id="问题的发现"><a href="#问题的发现" class="headerlink" title="问题的发现"></a>问题的发现</h2><p>我最初只是发现，代码运行时，每轮的progress bar一次比一次运行时间长，并且在第二轮会直接出现segmentation fault (core dumped)错误；当时并没有发现内存泄漏的问题。</p>
<h2 id="问问搜索引擎呢？"><a href="#问问搜索引擎呢？" class="headerlink" title="问问搜索引擎呢？"></a>问问搜索引擎呢？</h2><p>网上搜了一圈，搜索出来的结果全都在说是因为在循环中新建了图结点，但是经过排除并不是这个问题。。。</p>
<p>但是获得一个信息，也就是session执行时间越来越长，是内存泄漏的问题。</p>
<h2 id="是显卡的问题吗？"><a href="#是显卡的问题吗？" class="headerlink" title="是显卡的问题吗？"></a>是显卡的问题吗？</h2><p>假设镜像没有问题，那么是新显卡的问题吗？因为刚换了新服务器，我最初怀疑是Nvidia对显卡的支持问题，也就是显卡的问题，因为在RTX 3090上代码运行并没有问题。</p>
<p>首先，我跑过PyTorch的代码，没有任何问题。</p>
<p>但是经过测试，发现其他代码运行良好。</p>
<h2 id="是镜像的问题吗？"><a href="#是镜像的问题吗？" class="headerlink" title="是镜像的问题吗？"></a>是镜像的问题吗？</h2><p>在RTX 3090服务器上用的是20.12版本的镜像，这个镜像不支持新的RTX 4090显卡，于是在新服务器上用的是22.12版本的镜像。那么是因为新镜像的吗？</p>
<p>由于20.12版本的镜像不支持RTX 4090，只能在旧服务器上测试22.12版本的镜像，但是由于实验室网络问题，导致这个测试无法进行。。。</p>
<p>于是作罢，反正新服务器上的问题是一定要解决的，于是这个问题的答案貌似也没那么重要了。</p>
<h2 id="到底是哪儿耗时在一直增加？"><a href="#到底是哪儿耗时在一直增加？" class="headerlink" title="到底是哪儿耗时在一直增加？"></a>到底是哪儿耗时在一直增加？</h2><p>我想要知道是tensorflow的哪个操作的运行时间越来越长，通过网上搜索，发现了timeline工具。</p>
<p>例如如下代码，通过tensorflow自带的timeline工具记录每个op的执行时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> timeline</span><br><span class="line"></span><br><span class="line">run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</span><br><span class="line">run_metadata = tf.RunMetadata()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="comment"># session.run的时候加上options和run_metadata参数</span></span><br><span class="line">sess.run(output, options=run_options, run_metadata=run_metadata)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意只能记录一次session.run的timeline数据</span></span><br><span class="line"><span class="comment"># 因此想要比较不同迭代次数的执行时间，每次迭代都记录一个timeline的json文件</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">    ...</span><br><span class="line">    tl = timeline.Timeline(run_metadata.step_stats)</span><br><span class="line">    ctf = tl.generate_chrome_trace_format()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;timeline_<span class="subst">&#123;i&#125;</span>.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(ctf)</span><br><span class="line">    ...</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>然后打开edge://tracing/，载入timeline的json日志文件，通过比较不同迭代时候的执行时间，来找到是哪儿个操作非常耗时。</p>
<p>通过timeline找到session执行时每个op的耗时，发现rfft一次比一次耗时：<strong>第10次迭代和第120次迭代相比，session.run中rfft执行时间从0.2ms变成3.0ms</strong></p>
<p>其他op都没有这个问题，那么为什么rfft会随着迭代一次比一次耗时？并且内存一直泄漏？</p>
<h2 id="到底是哪儿内存泄漏？"><a href="#到底是哪儿内存泄漏？" class="headerlink" title="到底是哪儿内存泄漏？"></a>到底是哪儿内存泄漏？</h2><p>通过objgraph、pympler(这个工具比较直观)等等通过检查python对象引用及其内存占用的工具，没有发现任何异样，也就是都显示没有内存泄漏。</p>
<p>但是，通过psutil.virtual_memory发现内存确实是一直增加的。</p>
<p>可见内存泄漏是发生在比python解释器更底层的地方。</p>
<h2 id="终于发现问题：真的是nvidia-tensorflow的bug吗？！"><a href="#终于发现问题：真的是nvidia-tensorflow的bug吗？！" class="headerlink" title="终于发现问题：真的是nvidia-tensorflow的bug吗？！"></a>终于发现问题：真的是nvidia-tensorflow的bug吗？！</h2><p>为了测试是不是反复调用rfft会导致内存泄漏，写了如下代码进行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time     <span class="comment"># 用来检查session的执行时间</span></span><br><span class="line"><span class="keyword">import</span> psutil   <span class="comment"># 用来检查内存占用</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    frames = tf.random.uniform((<span class="number">144</span>, <span class="number">400</span>))</span><br><span class="line">    spec = tf.signal.rfft(frames, [<span class="number">512</span>, ])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">    session.run(tf.global_variables_initializer())</span><br><span class="line">    session.graph.finalize()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">        stime = time.time()</span><br><span class="line"></span><br><span class="line">        session.run(spec)       <span class="comment"># rfft computation</span></span><br><span class="line"></span><br><span class="line">        etime = time.time()</span><br><span class="line">        elapsed = etime - stime</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f&#x27;<span class="subst">&#123;i:05&#125;</span> Time: <span class="subst">&#123;elapsed&#125;</span>&#x27;</span>)</span><br><span class="line">            used_mem = psutil.virtual_memory().used                     <span class="comment"># memory usage</span></span><br><span class="line">            print(<span class="string">f&quot;<span class="subst">&#123;i:05&#125;</span> Memory used: <span class="subst">&#123;used_mem / <span class="number">1024</span> / <span class="number">1024</span>&#125;</span> Mb\n&quot;</span>) <span class="comment"># time cost</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">00000 Time: 0.04090452194213867</span><br><span class="line">00000 Memory used: 1823.8671875 Mb</span><br><span class="line"></span><br><span class="line">00500 Time: 0.0012400150299072266</span><br><span class="line">00500 Memory used: 1833.21875 Mb</span><br><span class="line"></span><br><span class="line">01000 Time: 0.0013303756713867188</span><br><span class="line">01000 Memory used: 1845.03125 Mb</span><br><span class="line"></span><br><span class="line">01500 Time: 0.001249551773071289</span><br><span class="line">01500 Memory used: 1856.59765625 Mb</span><br><span class="line"></span><br><span class="line">02000 Time: 0.001224517822265625</span><br><span class="line">02000 Memory used: 1868.40234375 Mb</span><br><span class="line"></span><br><span class="line">02500 Time: 0.0013511180877685547</span><br><span class="line">02500 Memory used: 1880.21484375 Mb</span><br><span class="line"></span><br><span class="line">03000 Time: 0.0013630390167236328</span><br><span class="line">03000 Memory used: 1892.02734375 Mb</span><br><span class="line"></span><br><span class="line">03500 Time: 0.0014331340789794922</span><br><span class="line">03500 Memory used: 1904.0859375 Mb</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<ul>
<li>发现如果一个<strong>loop里如果调用tf.rfft会导致内存一直增加，并且执行时间变慢</strong>！！！</li>
</ul>
<p>还发现一个有趣的事实，如果上面代码的<em>frames</em>变量是其他类型，则可能不会导致这个问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    frames = tf.zeros((<span class="number">144</span>, <span class="number">400</span>)) <span class="comment"># 或者ones</span></span><br><span class="line">    <span class="comment"># 或者 frames = tf.convert_to_tensor(numpy.random.random(144, 400))</span></span><br><span class="line">    <span class="comment"># 或者 frames = numpy.random.random(144, 400)</span></span><br><span class="line">    spec = tf.signal.rfft(frames, [<span class="number">512</span>, ])</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">00000 Time: 0.1534738540649414</span><br><span class="line">00000 Memory used: 1996.67578125 Mb</span><br><span class="line"></span><br><span class="line">00500 Time: 0.0013377666473388672</span><br><span class="line">00500 Memory used: 1996.953125 Mb</span><br><span class="line"></span><br><span class="line">01000 Time: 0.001354217529296875</span><br><span class="line">01000 Memory used: 1996.953125 Mb</span><br><span class="line"></span><br><span class="line">01500 Time: 0.0013136863708496094</span><br><span class="line">01500 Memory used: 1996.953125 Mb</span><br><span class="line"></span><br><span class="line">02000 Time: 0.0010759830474853516</span><br><span class="line">02000 Memory used: 1996.6796875 Mb</span><br><span class="line"></span><br><span class="line">02500 Time: 0.0010845661163330078</span><br><span class="line">02500 Memory used: 1996.6796875 Mb</span><br><span class="line"></span><br><span class="line">03000 Time: 0.0009725093841552734</span><br><span class="line">03000 Memory used: 1996.6796875 Mb</span><br><span class="line"></span><br><span class="line">03500 Time: 0.0011267662048339844</span><br><span class="line">03500 Memory used: 1996.6796875 Mb</span><br><span class="line"></span><br><span class="line">04000 Time: 0.0012097358703613281</span><br><span class="line">04000 Memory used: 1996.6484375 Mb</span><br><span class="line"></span><br><span class="line">04500 Time: 0.0013315677642822266</span><br><span class="line">04500 Memory used: 1996.6484375 Mb</span><br><span class="line"></span><br><span class="line">05000 Time: 0.0012805461883544922</span><br><span class="line">05000 Memory used: 1996.63671875 Mb</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>输入如果是numpy类型、或者tensorflow的const Tensor、tf.ones这类函数创建的Tensor，不会导致这个问题！！！</p>
<p>但是经过测试，如果输入类型是Variable和tf.random模块得到的对象会有这个问题。</p>
<ul>
<li>另外，除了rfft，其他时频变换方法如tf.fft, tf.ifft, tf.dct也有这个问题！！！</li>
</ul>
<h2 id="到底怎么办呢？"><a href="#到底怎么办呢？" class="headerlink" title="到底怎么办呢？"></a>到底怎么办呢？</h2><p>由于任务需要，输入只能是Variable类型的输入，因此一时间感觉不知道怎么办。</p>
<p>目前确定了不是自己代码的问题，由于不知道bug出现在哪儿一层，于是假设试着<strong>降tensorflow的版本到1.15.4</strong>，结果问题就解决了。</p>
<p>就这样吧。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>nvidia-tensorflow 1.15.5 的tf.signal模块的一些频域变换函数有bug。</p>
<p>提交issue：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/tensorflow/issues/76">https://github.com/NVIDIA/tensorflow/issues/76</a></p>

      
    </div>
    <footer class="article-footer">
	  
	  <!-- 百度分享 Start -->
	  <div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a><a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a></div>
	  <!-- 百度分享 End -->
    
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RTX-4090/" rel="tag">RTX 4090</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nvidia/" rel="tag">nvidia</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow-1-15/" rel="tag">tensorflow 1.15</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2/" rel="tag">内存泄露</a></li></ul>

	  

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/02/17/suibi/image_lowpass_filtering/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Image Low-pass Filtering Algorithms Ideal/Butterworth/Gaussian (PyTorch Implementation) 图像低通滤波算法PyTorch实现
        
      </div>
    </a>
  
  
    <a href="/2022/10/18/paper_notes/cvpr20_video_backdoor/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">CVPR2020 Clean-Label Backdoor Attacks on Video Recognition Models</div>
    </a>
  
</nav>

  
</article>



  <section id="comments" class="vcomment">

  </section>
</section>
        
          
  <div id="toc" class="toc-aside">
  <h2 class="toc-title">Contents</h2>
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E7%9A%84%E5%8F%91%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text">问题的发现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%97%AE%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%91%A2%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">问问搜索引擎呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%AF%E6%98%BE%E5%8D%A1%E7%9A%84%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F"><span class="toc-number">3.</span> <span class="toc-text">是显卡的问题吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%AF%E9%95%9C%E5%83%8F%E7%9A%84%E9%97%AE%E9%A2%98%E5%90%97%EF%BC%9F"><span class="toc-number">4.</span> <span class="toc-text">是镜像的问题吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%B0%E5%BA%95%E6%98%AF%E5%93%AA%E5%84%BF%E8%80%97%E6%97%B6%E5%9C%A8%E4%B8%80%E7%9B%B4%E5%A2%9E%E5%8A%A0%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">到底是哪儿耗时在一直增加？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%B0%E5%BA%95%E6%98%AF%E5%93%AA%E5%84%BF%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">到底是哪儿内存泄漏？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%88%E4%BA%8E%E5%8F%91%E7%8E%B0%E9%97%AE%E9%A2%98%EF%BC%9A%E7%9C%9F%E7%9A%84%E6%98%AFnvidia-tensorflow%E7%9A%84bug%E5%90%97%EF%BC%9F%EF%BC%81"><span class="toc-number">7.</span> <span class="toc-text">终于发现问题：真的是nvidia-tensorflow的bug吗？！</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%B0%E5%BA%95%E6%80%8E%E4%B9%88%E5%8A%9E%E5%91%A2%EF%BC%9F"><span class="toc-number">8.</span> <span class="toc-text">到底怎么办呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">9.</span> <span class="toc-text">总结</span></a></li></ol>
    
  </div>

<aside id="sidebar">

  
    
<div class="widget-wrap">
  <h3 class="widget-title">ABOUT ME</h3>
  <ul class="widget about-me">
    
    <li><img class="author" title="About me" src="/images/coffee_onetwo.jpg" /></li>
    
    
    <li>The must have been a door there in the wall, when I came in.</li>
    
  </ul>
</div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a><span class="category-list-count">14</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CTC/" style="font-size: 13px; color: #7dc3de">CTC</a> <a href="/tags/ChatGPT/" style="font-size: 13px; color: #7dc3de">ChatGPT</a> <a href="/tags/DFT/" style="font-size: 13px; color: #7dc3de">DFT</a> <a href="/tags/GAN/" style="font-size: 14.17px; color: #6dc1b9">GAN</a> <a href="/tags/PyTorch/" style="font-size: 13px; color: #7dc3de">PyTorch</a> <a href="/tags/RTX-4090/" style="font-size: 13px; color: #7dc3de">RTX 4090</a> <a href="/tags/ReLU/" style="font-size: 15.33px; color: #5dbe94">ReLU</a> <a href="/tags/Text-to-Image/" style="font-size: 13px; color: #7dc3de">Text-to-Image</a> <a href="/tags/Wav2Vec2/" style="font-size: 13px; color: #7dc3de">Wav2Vec2</a> <a href="/tags/Windows%E7%BC%96%E7%A8%8B/" style="font-size: 13px; color: #7dc3de">Windows编程</a> <a href="/tags/copyright/" style="font-size: 13px; color: #7dc3de">copyright</a> <a href="/tags/nvidia/" style="font-size: 13px; color: #7dc3de">nvidia</a> <a href="/tags/tensorflow-1-15/" style="font-size: 13px; color: #7dc3de">tensorflow 1.15</a> <a href="/tags/viterbi%E7%AE%97%E6%B3%95/" style="font-size: 13px; color: #7dc3de">viterbi算法</a> <a href="/tags/%E4%B8%AD%E5%BF%83%E5%8C%96%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/" style="font-size: 13px; color: #7dc3de">中心化差分隐私</a> <a href="/tags/%E4%BA%8C%E5%80%BC%E5%8C%96/" style="font-size: 13px; color: #7dc3de">二值化</a> <a href="/tags/%E4%BB%A3%E7%A0%81%E6%B3%A8%E5%85%A5/" style="font-size: 13px; color: #7dc3de">代码注入</a> <a href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/" style="font-size: 13px; color: #7dc3de">优化器</a> <a href="/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 13px; color: #7dc3de">优化算法</a> <a href="/tags/%E4%BD%8E%E9%80%9A%E6%BB%A4%E6%B3%A2/" style="font-size: 13px; color: #7dc3de">低通滤波</a> <a href="/tags/%E4%BD%8E%E9%80%9A%E6%BB%A4%E6%B3%A2%E5%99%A8/" style="font-size: 13px; color: #7dc3de">低通滤波器</a> <a href="/tags/%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2/" style="font-size: 13px; color: #7dc3de">内存泄露</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%A6%E4%B9%A0/" style="font-size: 13px; color: #7dc3de">分布式学习</a> <a href="/tags/%E5%89%8D%E5%90%91%E7%AE%97%E6%B3%95/" style="font-size: 13px; color: #7dc3de">前向算法</a> <a href="/tags/%E5%8D%8F%E4%BD%9C%E5%AD%A6%E4%B9%A0/" style="font-size: 13px; color: #7dc3de">协作学习</a> <a href="/tags/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC/" style="font-size: 13px; color: #7dc3de">双线性插值</a> <a href="/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/" style="font-size: 16.5px; color: #4dbc6f">可视化</a> <a href="/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7/" style="font-size: 17.67px; color: #3db94a">可解释性</a> <a href="/tags/%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/" style="font-size: 14.17px; color: #6dc1b9">后门攻击</a> <a href="/tags/%E5%90%8E%E9%97%A8%E6%A3%80%E6%B5%8B/" style="font-size: 13px; color: #7dc3de">后门检测</a> <a href="/tags/%E5%93%B2%E5%AD%A6/" style="font-size: 16.5px; color: #4dbc6f">哲学</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" style="font-size: 13px; color: #7dc3de">图像分割</a> <a href="/tags/%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/" style="font-size: 13px; color: #7dc3de">图片识别</a> <a href="/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 13px; color: #7dc3de">大语言模型</a> <a href="/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/" style="font-size: 16.5px; color: #4dbc6f">对抗攻击</a> <a href="/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/" style="font-size: 14.17px; color: #6dc1b9">对抗样本</a> <a href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" style="font-size: 13px; color: #7dc3de">对比学习</a> <a href="/tags/%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/" style="font-size: 13px; color: #7dc3de">差分隐私</a> <a href="/tags/%E5%BA%B7%E5%BE%B7/" style="font-size: 16.5px; color: #4dbc6f">康德</a> <a href="/tags/%E6%8F%90%E7%A4%BA%E6%B3%A8%E5%85%A5/" style="font-size: 13px; color: #7dc3de">提示注入</a> <a href="/tags/%E6%94%BF%E6%B2%BB%E7%BB%8F%E6%B5%8E%E5%AD%A6%E6%A6%82%E8%AE%BA/" style="font-size: 13px; color: #7dc3de">政治经济学概论</a> <a href="/tags/%E6%95%B0%E5%AD%97%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 15.33px; color: #5dbe94">数字信号处理</a> <a href="/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 14.17px; color: #6dc1b9">数字图像处理</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%8A%95%E6%AF%92/" style="font-size: 13px; color: #7dc3de">数据投毒</a> <a href="/tags/%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/" style="font-size: 13px; color: #7dc3de">本地化差分隐私</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 14.17px; color: #6dc1b9">机器学习</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E9%B2%81%E6%A3%92%E6%80%A7/" style="font-size: 13px; color: #7dc3de">模型鲁棒性</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px; color: #1db400">深度学习</a> <a href="/tags/%E7%89%88%E6%9D%83%E4%BF%9D%E6%8A%A4/" style="font-size: 13px; color: #7dc3de">版权保护</a> <a href="/tags/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/" style="font-size: 13px; color: #7dc3de">生成对抗网络</a> <a href="/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" style="font-size: 13px; color: #7dc3de">生成模型</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 18.83px; color: #2db725">神经网络</a> <a href="/tags/%E7%BA%AF%E7%B2%B9%E7%90%86%E6%80%A7%E6%89%B9%E5%88%A4/" style="font-size: 16.5px; color: #4dbc6f">纯粹理性批判</a> <a href="/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 13px; color: #7dc3de">联邦学习</a> <a href="/tags/%E8%A7%86%E9%A2%91/" style="font-size: 13px; color: #7dc3de">视频</a> <a href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 13px; color: #7dc3de">语言模型</a> <a href="/tags/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/" style="font-size: 13px; color: #7dc3de">语音识别</a> <a href="/tags/%E8%B6%8A%E7%8B%B1/" style="font-size: 13px; color: #7dc3de">越狱</a> <a href="/tags/%E8%B7%A8%E6%A8%A1%E6%80%81/" style="font-size: 13px; color: #7dc3de">跨模态</a> <a href="/tags/%E8%BF%87%E6%8B%9F%E5%90%88/" style="font-size: 13px; color: #7dc3de">过拟合</a> <a href="/tags/%E9%87%8D%E9%87%87%E6%A0%B7/" style="font-size: 13px; color: #7dc3de">重采样</a> <a href="/tags/%E9%87%8D%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95/" style="font-size: 13px; color: #7dc3de">重采样算法</a> <a href="/tags/%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86/" style="font-size: 13px; color: #7dc3de">阈值处理</a> <a href="/tags/%E9%99%8D%E9%87%87%E6%A0%B7/" style="font-size: 13px; color: #7dc3de">降采样</a> <a href="/tags/%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/" style="font-size: 15.33px; color: #5dbe94">隐私保护</a> <a href="/tags/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" style="font-size: 13px; color: #7dc3de">隐马尔可夫</a> <a href="/tags/%E9%9F%B3%E9%A2%91%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/" style="font-size: 14.17px; color: #6dc1b9">音频对抗攻击</a> <a href="/tags/%E9%9F%B3%E9%A2%91%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 13px; color: #7dc3de">音频预处理</a> <a href="/tags/%E9%A2%91%E8%B0%B1%E6%B7%B7%E5%8F%A0/" style="font-size: 13px; color: #7dc3de">频谱混叠</a> <a href="/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" style="font-size: 13px; color: #7dc3de">马尔可夫</a>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">October 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/25/suibi/interpret_dft/">Note on understanding DFT</a>
          </li>
        
          <li>
            <a href="/2023/10/03/reading_notes/pure_reason_critique_tologic/">纯粹理性批判阅读笔记-先验逻辑：导言</a>
          </li>
        
          <li>
            <a href="/2023/09/23/suibi/optimizers/">神经网络的优化器们</a>
          </li>
        
          <li>
            <a href="/2023/09/23/reading_notes/pure_reason_critique_time/">纯粹理性批判阅读笔记-感性纯直观：时间</a>
          </li>
        
          <li>
            <a href="/2023/09/17/suibi/jailbroken_paper/">Jailbroken文章阅读</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
  <h3 class="widget-title">Links</h3>
  <ul class="widget">
    
    <li><a href="https://qgrain.github.io/" target="_BLANK">Zhiyu&#39;s Blog</a></li>
    
    <li><a href="https://www.marxists.org/chinese/marx/index.htm" target="_BLANK">中文马克思主义文库</a></li>
    
    <li><a href="https://www.zdic.net/" target="_BLANK">汉典</a></li>
    
  </ul>
</div>


  

</aside>

        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 cassiniwei@outlook.com<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> and Theme by <a href="https://github.com/howiefh/hexo-theme-landscape-f" target="_blank" title="Landscape-F">Landscape-F</a>
    
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a target="_blank" rel="noopener" href="https://space.bilibili.com/85520747" class="mobile-nav-link">bilibili</a>
  
</nav>
    


<!-- 百度分享 start -->
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","douban","bdysc","sqq","qq","hi","baidu","huaban","youdao","sdo","mail","xg","diandian","fx","copy","print"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["qzone","tsina","tqq","renren","weixin"],"viewText":"分享到","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
<!-- 百度分享 end -->



<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>



<div class="bottom-btn">

	<a class="icon-gotop" href="javascript:void(0)" title="返回顶部"></a>
	
<script src="/js/gotop.js"></script>



	<a class="icon-toc-toggle" href="javascript:void(0)" title="文章目录"></a>
	
<script src="/js/toc_aside_toggle.js"></script>


</div>



<script src="/js/script.js"></script>









  
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

<script>
    var GUEST_INFO = ['nick','mail','link'];
    var guest_info = 'nick,mail,link'.split(',').filter(function(item){
        return GUEST_INFO.indexOf(item) > -1
    });
    var notify = 'true' == true;
    var verify = 'true' == true;
    new Valine({
        el: '.vcomment',
        notify: notify,
        verify: verify,
        appId: "t2m58rhzOMoJTD21pEaJ5gHJ-MdYXbMMI",
        appKey: "FCXmx3W5LaZfCPys3TBd1uul",
        placeholder: "New comment notification enabled! Feel free to comment.",
        pageSize:'10',
        avatar:'mm',
        lang:''
    });
</script>


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>


</body>
</html>
